{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Amy Weng\n",
    "\n",
    "Code adapted from https://towardsdatascience.com/word2vec-explained-49c52b4ccb71#:~:text=CBOW%20%28continuous%20bag%20of%20words%29%20and%20the%20skip-gram,and%20try%20to%20predict%20the%20missing%20one.%20Resources. \n",
    "\n",
    "https://towardsdatascience.com/word-embeddings-and-embedding-projector-of-tensorflow-c946b98c9b3f\n",
    "\n",
    "Word2Vec Continuous Bag of Words Model Word Embedding and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from words import remove_stopwords\n",
    "\n",
    "from gensim.scripts import word2vec2tensor\n",
    "from gensim.models import Word2Vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "def embed(df,title):\n",
    "    data = df.text.values.tolist()\n",
    "    # preprocess and remove stopwords\n",
    "    data = remove_stopwords(data)\n",
    "    model = w2v(data,min_count=3,sg = 1,window=7)     \n",
    "    model.wv.save_word2vec_format('/home/rapiduser/Materials/embeddings/'+title+'.model')\n",
    "    return(model)\n",
    "\n",
    "def similar(model,word):\n",
    "    if word in model.wv.key_to_index.keys():\n",
    "        words = []\n",
    "        for w, s in model.wv.most_similar(word):\n",
    "            words.append(w)\n",
    "        print(word + ': '+' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCSV = '/home/rapiduser/Materials/topic model/publica/eic_monopoly.csv'\n",
    "\n",
    "readFile = pd.read_csv(myCSV)\n",
    "\n",
    "# read text information into a dataframe\n",
    "publica_eic_monopoly = embed(readFile,'publica_eic_monopoly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monopolies: retraints retrained authorities statutes restraints repugnant inhibitions aith imply enacting\n",
      "monopolium: godwini theses gonformity imperantium pomfret bodington circul rous hurst indos\n",
      "monopoly: retraints restraints ingroing restraint uing retrained excluive engrossing limitation restores\n",
      "monopolise: avenge engross prejudiced inconvenient supposing ingroing poil alledg arguing moreover\n",
      "monopolising: foreignof poil interruption marts mediums michievous impoverishing amends includes coniders\n",
      "monopolists: monopolits cumber impoverished unpeakable impoverihed unmanufactured expending stretching inasmuch drilling\n",
      "monopolizers: patterns ditinctly incorporating bono suppoition slighted disadvantageous ubervient manufacturer diadvantageous\n",
      "monopolised: solely confining avenge restraining separate conjunction interloping admiion negotiated obtruction\n",
      "monoopolies: patious praedicta dienting nighet conspiring herbsthe bandittoes cruising acquaints adoring\n",
      "monopolits: monopolists cumber impoverished unpeakable unmanufactured straining inamuch workmanhip engrossers shooing\n",
      "monopolers: scrivans wharfage normanby denominates brampore quatuor lougaroos island ariani proelyting\n"
     ]
    }
   ],
   "source": [
    "from words import monopoly\n",
    "m = monopoly.split('|')\n",
    "for w in m:\n",
    "    similar(publica_eic_monopoly,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts import word2vec2tensor\n",
    "tensor_dir='/home/rapiduser/Materials/tensor'\n",
    "word2vec2tensor.word2vec2tensor(\n",
    "    '/home/rapiduser/Materials/embeddings/publica_eic_monopoly.model',\n",
    "    tensor_dir+'/publica_eic_monopoly',\n",
    "    binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "plt.style.use('ggplot')\n",
    "def visualizePCA(emb_df):\n",
    "    pca = PCA(n_components=2, random_state=7)\n",
    "    pca_mdl = pca.fit_transform(emb_df)\n",
    "\n",
    "    emb_df_PCA = (\n",
    "        pd.DataFrame(\n",
    "            pca_mdl,\n",
    "            columns=['x','y'],\n",
    "            index = emb_df.index\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(6,6))\n",
    "\n",
    "    plt.scatter(\n",
    "        x = emb_df_PCA['x'],\n",
    "        y = emb_df_PCA['y'],\n",
    "        s = 0.5,\n",
    "        color = 'maroon',\n",
    "        alpha = 0.5\n",
    "    )\n",
    "\n",
    "    plt.xlabel('PCA-1')\n",
    "    plt.ylabel('PCA-2')\n",
    "    plt.title('PCA Visualization')\n",
    "    plt.plot()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
