{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Amy Weng\n",
    "\n",
    "Word2Vec Continuous Skip-Gram Model Word Embedding and Visualization\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from words import remove_stopwords\n",
    "from gensim.scripts import word2vec2tensor\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "\n",
    "def embed(df,title):\n",
    "    data = df.text.values.tolist()\n",
    "    # preprocess and remove stopwords\n",
    "    data = remove_stopwords(data)\n",
    "    # Train a bigram detector.\n",
    "    bigram_transformer = Phrases(data)\n",
    "    # Train a skip-gram model with the bigram detector \n",
    "    model = Word2Vec(bigram_transformer[data], min_count=5,sg=1)\n",
    "    # save model so we can reload later  \n",
    "    model.save('/home/rapiduser/Materials/embeddings/'+title+'.model')\n",
    "    # save model in a format that can be converted to tensor TSV \n",
    "    model.wv.save_word2vec_format('/home/rapiduser/Materials/embeddings/tensor/'+title+'.model')\n",
    "    return(model)\n",
    "\n",
    "def similar(model,word,num):\n",
    "    if word in model.wv.key_to_index.keys():\n",
    "        words = []\n",
    "        first = 0\n",
    "        score = 0\n",
    "        for w, s in model.wv.most_similar(word,topn=num):\n",
    "            if first == 0:\n",
    "                score = s\n",
    "                first += 1\n",
    "            words.append(w)\n",
    "        #     words.append((w,s))\n",
    "        # print(word + ': ',words)\n",
    "        print(word + ': '+' '.join(words))\n",
    "        print('The most similar n-gram has cosine distance '+str(score))\n",
    "\n",
    "def comparePair(model,word1,word2):\n",
    "    if word1 and word2 in model.wv.key_to_index.keys():\n",
    "        print('Cosine similarity between ' + word1 + ' and ' + word2 + ': ',model.wv.similarity(word1, word2))\n",
    "\n",
    "def tensor(f_name):\n",
    "    word2vec2tensor.word2vec2tensor(\n",
    "        '/home/rapiduser/Materials/embeddings/tensor/'+f_name+'.model',\n",
    "        '/home/rapiduser/ECBC-21-22/Text_Files/Embeddings TSV/'+f_name,\n",
    "        binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCSV = '/home/rapiduser/Materials/topic model/publica/eic_monopoly.csv'\n",
    "\n",
    "readFile = pd.read_csv(myCSV)\n",
    "\n",
    "# read text information into a dataframe\n",
    "publica_eic_monopoly = embed(readFile,'publica_eic_monopoly')\n",
    "tensor('publica_eic_monopoly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publica_eic_monopoly = Word2Vec.load('/home/rapiduser/Materials/embeddings/publica_eic_monopoly.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCSV = '/home/rapiduser/Materials/topic model/publica/eic.csv'\n",
    "readFile = pd.read_csv(myCSV)\n",
    "# read text information into a dataframe\n",
    "publica_eic = embed(readFile,'publica_eic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCSV = '/home/rapiduser/Materials/topic model/religio/eic.csv'\n",
    "readFile = pd.read_csv(myCSV)\n",
    "# read text information into a dataframe\n",
    "religio_eic = embed(readFile,'religio_eic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corruption: multiply influenc simplicity_original health_infirmity sovereign_authority arts_industry reasonableness endeavour_show subjecting utility genuine task_masters became_tyrants annihilate humane_nature\n",
      "The most similar n-gram has cosine distance 0.854022741317749\n",
      "monopoly: joynt_stocks monopolies reas joint_stocks retraints advisable wisdom_nation enlarging_trade national_advantage regulated_companies exclusive_others equitable excluive_others etablihment_new enable_carry\n",
      "The most similar n-gram has cosine distance 0.8654782176017761\n",
      "monopolies: retraints erected_maintenance freedom_trade ordering_trade restraints restrictions monopoly grant_sole legally_excluded companies_societies excluding_others prohibitory aerted patents_granted retraining\n",
      "The most similar n-gram has cosine distance 0.9139752984046936\n",
      "body_politic: corporate bodies_politic ingroing whole_buying excluding_others ealing_uing selling_using erected_maintenance companies_societies politic_capacity monopolies engrossing eential bank_corporation prop_person\n",
      "The most similar n-gram has cosine distance 0.8317705988883972\n",
      "odious: arrogance provoking malicious audacious villainies vile imputation impious myteries ingratitude humane humanity abhorred human applaud\n",
      "The most similar n-gram has cosine distance 0.9085683226585388\n",
      "east_india: bill_restraining company eat_india answ new_subscribers east_indies actions_pretences merchant_adventurers material_objections unlawful_practices turkey_company african_company african_companies list_damages india_company\n",
      "The most similar n-gram has cosine distance 0.7141093015670776\n"
     ]
    }
   ],
   "source": [
    "similar(publica_eic,'corruption',15)\n",
    "similar(publica_eic,'monopoly',15)\n",
    "similar(publica_eic,'monopolies',15)\n",
    "similar(publica_eic,'body_politic',15)\n",
    "similar(publica_eic,'odious',15)\n",
    "similar(publica_eic,'east_india',15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monopolies: laws_land freedom_trade retrained restrain restrained retrain exclude_others wrong legally privilege\n",
      "The most similar n-gram has cosine distance 0.9599364995956421\n",
      "monopoly: exclusive depending regulated_company joint_stocks monopolies retrained privilege wrong majeties_subjects allege\n",
      "The most similar n-gram has cosine distance 0.9274605512619019\n",
      "monopolise: allegations advantages_profits ingroed naturally_follow befalls free_encumbrances trengthen fact_allege corporation_excluding default_paying\n",
      "The most similar n-gram has cosine distance 0.9918798804283142\n",
      "monopolising: enriched greater_prices remote_parts riches_strength rates_exchange feel_effects encreaing increase_navigation increase_seamen attempting\n",
      "The most similar n-gram has cosine distance 0.9856666326522827\n",
      "monopolizers: lil vat_extent butchers served_apprentice tillage stil betowed consuming afford_gains trademan\n",
      "The most similar n-gram has cosine distance 0.991474449634552\n",
      "monopolised: olely forces_forts diffuive majeties_revenue solely extenive timely_prevented divided_amongt restraining diffusive\n",
      "The most similar n-gram has cosine distance 0.9849039316177368\n"
     ]
    }
   ],
   "source": [
    "from words import monopoly\n",
    "m = monopoly.split('|')\n",
    "for w in m:\n",
    "    similar(publica_eic_monopoly,w,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms1 = 'body_politic|body_politick|public_utility|public_affairs|wicked|illegal|engrossing|odious|evil|repugnant|arbitrary|popery|papist|bribery|remedies|remedy|monopoly_evils|circulation|disease|wasting|waste'\n",
    "terms2 = 'monopoly|monopolies|monopolise|monopolising|monopolizers|monopolised|corruption|corrupt|corrupting'\n",
    "list1 = terms1.split('|')\n",
    "list2 = terms2.split('|')\n",
    "for word1 in list1: \n",
    "    for word2 in list2: \n",
    "        comparePair(publica_eic_monopoly,word1,word2)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption = 'consumption|consume|consuming|consumed|conume|conumption|conuming|conumed'\n",
    "c = consumption.split('|')\n",
    "for w in c:\n",
    "    similar(publica_eic_monopoly,w,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruption = 'corruption|corrupt|corrupted|corruptions|corrupting'\n",
    "corrupt = corruption.split('|')\n",
    "for w in corrupt:\n",
    "    similar(publica_eic_monopoly,w,10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
