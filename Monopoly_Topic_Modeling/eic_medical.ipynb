{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c71dbcb",
   "metadata": {},
   "source": [
    "Author: Amy Weng\n",
    "\n",
    "This code is for running named entity recognition (NER) using spaCy on individual Restoration-era texts that mention monopoly, East India Company, and other terms we are interested in. \n",
    "\n",
    "FIXES: I filtered out texts with the same title from my eicFolder using \"east india company|east-india company|east-india-company\" this time. I got much more results after including the latter two, which emphasizes the importance of researchers knowing the linguistic conventions of the time. \n",
    "\n",
    "---------------------\n",
    "CHALLENGES: I tried to run the algorithm on the entire monopolyFolder, but NER w/ spaCy has a max character limit of 1 million (see last cell on this page). The code ran for nearly 20 minutes before failing. What I tried next was to filter through the texts further for mentions of the East India Company to narrow down the number of texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9417b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import sys\n",
    "# !{sys.executable} -m pip install spacy\n",
    "# !{sys.executable} -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "directory = r'C:\\Users\\amycw\\Desktop\\ecbc research'\n",
    "monopolyFolder = directory + \"\\\\\" + 'monopoly_eebo'\n",
    "eicFolder = directory + \"\\\\\"+'eic_monopoly_eebo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c82632b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of texts that mention the EIC is 61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eicCount = 0;\n",
    "eicFile = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\eic titles and filenames.txt', \"a\")\n",
    "\n",
    "for name in os.listdir(monopolyFolder):\n",
    "    readFile = pd.read_csv(monopolyFolder + \"\\\\\" + name)\n",
    "    num_rows = readFile.index\n",
    "    num_texts = len(num_rows)\n",
    "    for i in range(num_texts):\n",
    "        data = readFile[i:(i+1)]\n",
    "        text = data.iloc[0].text\n",
    "        title = data.iloc[0].title\n",
    "        eic = re.compile(\"east india company|east-india company|east-india-company\")\n",
    "        if (re.search(eic, text) != None):\n",
    "            moveFile = eicFolder + \"\\\\\" + name\n",
    "            data.to_csv(moveFile, index=False)\n",
    "            eicFile.write(name + \"\\n\" + title +\"\\n\")\n",
    "            eicCount +=1\n",
    "print(\"the number of texts that mention the EIC is \"+str(eicCount)+\"\\n\")\n",
    "eicFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96bceee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of files that have medical terms is 61\n"
     ]
    }
   ],
   "source": [
    "# medCount = 0; \n",
    "allMedTerms = []\n",
    "medCount = 0\n",
    "medFile = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\medical_eic_monopoly.txt', \"a\")\n",
    "for name in os.listdir(eicFolder):\n",
    "    readFile = pd.read_csv(eicFolder + \"\\\\\" + name)\n",
    "    num_rows = readFile.index\n",
    "    num_texts = len(num_rows)\n",
    "    for i in range(num_texts):\n",
    "        data = readFile[i:(i+1)]\n",
    "        text = data.iloc[0].text\n",
    "        lexicon = re.compile(\"corruption|consumption|body politick|corrupt|consume|wasting|waste|blood|physician|decay|body|canker|cancer|disease|illness|remedy|cure|sickness|hepatitis|fever|spirit|brain|mind|vital|therapeutic|degeneration|degenerate|plague|smells|putrid|bad|tetrid|breathing|wholesome|healthy|unhealthy|sane|insane|nervous|languish|faculties|enfeeblement|drinks|tuberculosis|constitution|bile|black bile|yellow bile|phlegm|lung|sores|fog|smoke|diagnosis|prognosis|fiber|atrophy|morbid|mortal|mortality|nerves|inanition|defect|distemper|swelling|upset|stomach|cough|exercise|unwholesome|evacuation|fatal|fatality|vessels|hemmorrhage|bleeding|bleed|melancholy|diabetes|asthma|vomit|opiate|opium|ulcer|envy|jealousy|spoil|liver|vein|supple|heart|mouth|cured|pox\")\n",
    "        if (re.search(lexicon, text) != None):\n",
    "            for w in (re.findall(lexicon, text)):\n",
    "                allMedTerms.append(w)\n",
    "                medFile.write(w+\" \")\n",
    "        medCount+=1\n",
    "print(\"the number of files that have medical terms is \"+str(medCount))\n",
    "medFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79fe4bdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cure', 896)\n",
      "('body', 720)\n",
      "('liver', 542)\n",
      "('blood', 482)\n",
      "('spirit', 376)\n",
      "('mind', 369)\n",
      "('heart', 284)\n",
      "('bad', 266)\n",
      "('mouth', 252)\n",
      "('constitution', 235)\n",
      "('remedy', 227)\n",
      "('consumption', 165)\n",
      "('disease', 159)\n",
      "('exercise', 149)\n",
      "('consume', 134)\n",
      "('spoil', 128)\n",
      "('mortal', 121)\n",
      "('physician', 116)\n",
      "('defect', 112)\n",
      "('waste', 100)\n",
      "('decay', 100)\n",
      "('plague', 97)\n",
      "('fatal', 89)\n",
      "('corrupt', 74)\n",
      "('corruption', 60)\n",
      "('melancholy', 58)\n",
      "('jealousy', 55)\n",
      "('brain', 47)\n",
      "('sickness', 45)\n",
      "('languish', 44)\n",
      "('degenerate', 44)\n",
      "('fever', 41)\n",
      "('distemper', 41)\n",
      "('stomach', 35)\n",
      "('vessels', 32)\n",
      "('supple', 32)\n",
      "('wasting', 30)\n",
      "('envy', 29)\n",
      "('lung', 26)\n",
      "('wholesome', 25)\n",
      "('vein', 23)\n",
      "('bleeding', 19)\n",
      "('breathing', 12)\n",
      "('vital', 11)\n",
      "('smoke', 11)\n",
      "('pox', 11)\n",
      "('bile', 11)\n",
      "('swelling', 10)\n",
      "('fog', 8)\n",
      "('cough', 8)\n",
      "('bleed', 8)\n",
      "('vomit', 7)\n",
      "('sores', 7)\n",
      "('smells', 7)\n",
      "('nerves', 7)\n",
      "('faculties', 7)\n",
      "('drinks', 5)\n",
      "('sane', 4)\n",
      "('canker', 4)\n",
      "('unwholesome', 3)\n",
      "('opium', 3)\n",
      "('putrid', 2)\n",
      "('phlegm', 2)\n",
      "('cancer', 2)\n",
      "('healthy', 1)\n",
      "('atrophy', 1)\n"
     ]
    }
   ],
   "source": [
    "uniqueMed = {} \n",
    "terms = []\n",
    "for w in allMedTerms:\n",
    "    if w not in terms: \n",
    "        uniqueMed[w] = 1\n",
    "        terms.append(w)\n",
    "    else: \n",
    "        uniqueMed[w] = uniqueMed[w] + 1 \n",
    "\n",
    "for pair in sorted (uniqueMed.items(), key=lambda kv:(kv[1], kv[0]), reverse=True):\n",
    "    print(str(pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b6c6d6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the history of england giving a true and impartial account of the most considerable transactions in church and state in peace and war during the reigns of all the kings and queens from the coming of julius caesar into britain with an account of all plots conspiracies insurrections and rebellions likewise a relation of the wonderful prodigies to the year together with a particular description of the rarities in the several counties of england and wales with exact maps of each county by john seller \n",
      "\n",
      "1185040\n"
     ]
    }
   ],
   "source": [
    "# check for files that are bigger than 1000000 \n",
    "for file in os.listdir(eicFolder):\n",
    "    entries = pd.read_csv(eicFolder + \"\\\\\" + file)\n",
    "    num_rows = entries.index\n",
    "    num_texts = len(num_rows)\n",
    "    for i in range(num_texts):\n",
    "        data = entries[i:(i+1)]\n",
    "        name = tooBig + \"\\\\\" + file\n",
    "        text = data.iloc[0].text\n",
    "        title = data.iloc[0].title\n",
    "        if (len(text)>=1000000):\n",
    "            print(title+\"\\n\")\n",
    "            print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "507b3c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running named entity recognition using spaCy on eicFolder (45 texts in total)\n",
    "#took 13 minutes \n",
    "nerFile = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\namedEntityRecognition.txt', \"a\")\n",
    "ner = spacy.load('en_core_web_sm')\n",
    "for fileName in os.listdir(eicFolder):\n",
    "    readFile = pd.read_csv(eicFolder + \"\\\\\" + fileName)\n",
    "    num_texts = len(readFile.index)\n",
    "    for i in range(num_texts):\n",
    "        data = readFile[i:(i+1)]\n",
    "        text = data.iloc[0].text\n",
    "        entities = ner(text)\n",
    "        for ent in entities.ents:\n",
    "            nerFile.write(ent.text+\" \"+ent.label_+\"\\n\")\n",
    "nerFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "526c4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabetize and sort entities by frequency\n",
    "\n",
    "nerFile = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\namedEntityRecognition.txt', \"r\")\n",
    "SortedNER = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\sortedNamedEntityRecognition.txt',\"a\")\n",
    "SortedNER.write(\"format is (<name> <TYPE>, <frequency>)\\n\")\n",
    "uniqueEnts = {} \n",
    "entities = []\n",
    "indices = []\n",
    "entities = nerFile.read().split(\"\\n\")\n",
    "for ent in entities:\n",
    "    if ent not in indices: \n",
    "        uniqueEnts[ent] = 1\n",
    "        indices.append(ent)\n",
    "    else: \n",
    "        uniqueEnts[ent] = uniqueEnts[ent] + 1 \n",
    "alphabetizedEnts = {}\n",
    "for ent in sorted(uniqueEnts): \n",
    "    alphabetizedEnts[ent]=uniqueEnts[ent] \n",
    "for pair in sorted (alphabetizedEnts.items(), key=lambda kv:(kv[1], kv[0]), reverse=True):\n",
    "    SortedNER.write(str(pair)+\"\\n\")\n",
    "SortedNER.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18770509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#break the too big file into two smaller ones \n",
    "\n",
    "breakFile = r'C:\\Users\\amycw\\Desktop\\ecbc research\\eic_monopoly_eebo\\A5_P4_3070.csv'\n",
    "entries= pd.read_csv(breakFile)\n",
    "half = []\n",
    "otherHalf = []\n",
    "count = 0 \n",
    "string = \"\"\n",
    "other = \"\"\n",
    "for i in range(len(entries.index)):\n",
    "    d = entries[i:(i+1)]\n",
    "    text = d.iloc[0].text\n",
    "    words = text.split(\" \")\n",
    "    for w in words: \n",
    "        if (count <= (len(words)/2)):\n",
    "            half.append(w)\n",
    "            count+=1\n",
    "        else:\n",
    "            otherHalf.append(w)\n",
    "for w in half:\n",
    "    string = string + w + \" \"\n",
    "for w in otherHalf: \n",
    "    other = other + w + \" \"\n",
    "\n",
    "df1 = pd.DataFrame([(d.iloc[0].index, d.iloc[0].title,d.iloc[0].author,d.iloc[0].publisher,d.iloc[0].date,string)],\n",
    "           columns=('index', 'title','author','publisher','date','text'))\n",
    "df2 = pd.DataFrame([(d.iloc[0].index, d.iloc[0].title,d.iloc[0].author,d.iloc[0].publisher,d.iloc[0].date,other)],\n",
    "           columns=('index', 'title','author','publisher','date','text'))\n",
    "df1.to_csv(eicFolder + \"\\\\\" + 'A5_P4_3070_1.csv', index=False)\n",
    "df2.to_csv(eicFolder + \"\\\\\" + 'A5_P4_3070_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a57cc0fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1449451 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-19008bcfbc4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadFile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mentities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    999\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m         \"\"\"\n\u001b[1;32m-> 1001\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1002\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1089\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1092\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE866\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1079\u001b[0m         \"\"\"\n\u001b[0;32m   1080\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1082\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: [E088] Text of length 1449451 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "# failed attempt \n",
    "nerFile = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\monopoly_NER.txt', \"a\")\n",
    "nerFile.write(\"format is (name type, frequency)\\n\")\n",
    "\n",
    "ner = spacy.load('en_core_web_sm')\n",
    "uniqueEnts = {} \n",
    "for fileName in os.listdir(monopolyFolder):\n",
    "    readFile = pd.read_csv(monopolyFolder + \"\\\\\" + fileName)\n",
    "    num_rows = readFile.index\n",
    "    num_texts = len(num_rows)\n",
    "    for i in range(num_texts):\n",
    "        data = readFile[i:(i+1)]\n",
    "        text = data.iloc[0].text\n",
    "        entities = ner(text)\n",
    "        for ent in entities.ents:\n",
    "            idx = ent.text+\" \"+ent.label_\n",
    "            if ent not in uniqueEnts: \n",
    "                uniqueEnts[idx] = 1\n",
    "            else: \n",
    "                uniqueEnts[idx] += 1 \n",
    "alphabetizedEnts = {}\n",
    "for ent in sorted(uniqueEnts): \n",
    "    alphabetizedEnts[ent]=uniqueEnts[ent] \n",
    "for pair in sorted (alphabetizedEnts.items(), key=lambda kv:(kv[1], kv[0]), reverse=True):\n",
    "    nerFile.write(str(pair)+\"\\n\")\n",
    "nerFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
