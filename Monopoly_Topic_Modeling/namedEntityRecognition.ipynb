{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c71dbcb",
   "metadata": {},
   "source": [
    "This code is for running named entity recognition (NER) using spaCy on individual Restoration-era texts that mention monopoly, East India Company, and other terms we are interested in. \n",
    "* NER: https://www.geeksforgeeks.org/python-named-entity-recognition-ner-using-spacy/\n",
    "* Sorting by val: https://www.geeksforgeeks.org/python-sort-python-dictionaries-by-key-or-value/\n",
    "\n",
    "CHALLENGES: I tried to run the algorithm on the entire monopolyFolder, but NER w/ spaCy has a max character limit of 1 million. The code ran for nearly 20 minutes before failing. What I tried next was to filter through the texts further for mentions of the East India Company to narrow down the number of texts, which resulted in 24 texts. One of the texts had more than 1 million characters, so I split it in half into two. I then ran NER on the eicFolder. I sorted the NER output by frequency and in alphabetical order. \n",
    "\n",
    "other notes: \n",
    "* every single Restoration monopoly EIC file (24 in total; the medCount prints 25 because I divided one file into two) contains some mentions of words within my  medical lexicon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9417b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import sys\n",
    "# !{sys.executable} -m pip install spacy\n",
    "# !{sys.executable} -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "directory = r'C:\\Users\\amycw\\Desktop\\ecbc research'\n",
    "monopolyFolder = directory + \"\\\\\" + 'monopoly_eebo'\n",
    "tooBig = directory + \"\\\\\" + 'tooBig'\n",
    "eicFolder = directory + \"\\\\\"+'eic_monopoly_eebo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c82632b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eicCount = 0;\n",
    "\n",
    "for name in os.listdir(monopolyFolder):\n",
    "    readFile = pd.read_csv(monopolyFolder + \"\\\\\" + name)\n",
    "    num_rows = readFile.index\n",
    "    num_texts = len(num_rows)\n",
    "    for i in range(num_texts):\n",
    "        data = readFile[i:(i+1)]\n",
    "        text = data.iloc[0].text\n",
    "        eic = re.compile(\"East India Company|east india company|east indies\")\n",
    "        if (re.search(eic, text) != None):\n",
    "#             moveFile = eicFolder + \"\\\\\" + name\n",
    "#             data.to_csv(moveFile, index=False)\n",
    "            eicCount +=1\n",
    "print(str(eicCount)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "96bceee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# medCount = 0; \n",
    "allMedTerms = []\n",
    "for name in os.listdir(eicFolder):\n",
    "    readFile = pd.read_csv(eicFolder + \"\\\\\" + name)\n",
    "    num_rows = readFile.index\n",
    "    num_texts = len(num_rows)\n",
    "    for i in range(num_texts):\n",
    "        data = readFile[i:(i+1)]\n",
    "        text = data.iloc[0].text\n",
    "        title = data.iloc[0].title\n",
    "        lexicon = re.compile(\"corruption|consumption|body politick|corrupt|consume|wasting|waste|blood|physician|decay|body|canker|disease|illness|remedy|cure|sickness|hepatitis|fever|air|life|death|spirit|brain|mind|vital|humor|longevity|therapeutic|nature|degeneration|degenerate|plague|smells|putrid|bad|tetrid|breathing|wholesome|healthy|unhealthy|sane|insane|nervous|languish|faculties|enfeeblement|drinks|tuberculosis|constitution|bile|black bile|yellow bile|phlegm|lung|sores|fog|smoke|diagnosis|prognosis|fiber|atrophy|morbid|mortal|mortality|nerves|inanition|defect|distemper|swelling|upset|stomach|cough|exercise|unwholesome|diet|evacuation|fatal|fatality|vessels|hemmorrhage|bleeding|bleed|melancholy|diabetes|asthma|sweat|serum|water|fire|vomit|opiate|opium|ulcer|envy|jealousy|spoil|liver|vein|wear|joint|supple|heart|mouth|shaking|urine|cured|pox\")\n",
    "        if (re.search(lexicon, text) != None):\n",
    "            for w in (re.findall(lexicon, text)):\n",
    "                allMedTerms.append(w)\n",
    "#             print(\"\\t\" + title + \"\\n\")\n",
    "#             print(re.findall(lexicon, text))\n",
    "#             print(\"\\n\")\n",
    "#             medCount += 1\n",
    "# print(str(medCount)+\"\\n\")\n",
    "# print(allMedTerms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "79fe4bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decay': 81, 'remedy': 111, 'liver': 293, 'cure': 479, 'bad': 236, 'air': 1198, 'nature': 406, 'joint': 256, 'body': 591, 'plague': 55, 'sickness': 34, 'corrupt': 85, 'mind': 321, 'spirit': 418, 'death': 520, 'constitution': 149, 'consume': 87, 'consumption': 91, 'exercise': 106, 'spoil': 81, 'disease': 68, 'defect': 60, 'envy': 26, 'physician': 50, 'life': 786, 'swelling': 14, 'melancholy': 24, 'heart': 410, 'blood': 302, 'diet': 69, 'wear': 211, 'mortal': 131, 'water': 589, 'wholesome': 24, 'vein': 17, 'fire': 252, 'waste': 71, 'mouth': 230, 'corruption': 34, 'degenerate': 29, 'faculties': 7, 'jealousy': 33, 'fatal': 46, 'sores': 6, 'fog': 12, 'vessels': 58, 'languish': 22, 'nerves': 4, 'ulcer': 2, 'distemper': 29, 'stomach': 25, 'breathing': 7, 'lung': 19, 'sweat': 30, 'brain': 31, 'drinks': 7, 'smoke': 15, 'sane': 4, 'fever': 20, 'shaking': 4, 'bile': 25, 'pox': 4, 'wasting': 11, 'supple': 13, 'bleeding': 4, 'vomit': 9, 'cough': 4, 'canker': 4, 'urine': 5, 'opium': 6, 'smells': 10, 'vital': 4, 'phlegm': 2, 'unwholesome': 11, 'putrid': 2, 'bleed': 1, 'serum': 1, 'healthy': 15, 'unhealthy': 2, 'humor': 1}\n"
     ]
    }
   ],
   "source": [
    "uniqueMed = {} \n",
    "terms = []\n",
    "for w in allMedTerms:\n",
    "    if w not in terms: \n",
    "        uniqueMed[w] = 1\n",
    "        terms.append(w)\n",
    "    else: \n",
    "        uniqueMed[w] = uniqueMed[w] + 1 \n",
    "print(uniqueMed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b6c6d6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geography rectified or a description of the world in all its kingdoms provinces countries islands cities towns seas rivers baize capes ports their ancient and present names inhabitants situations histories customs governments as also their commodities coins weights and measures compared with those at london illustrated with seventy six maps the whole work performed according to the more accurate observations and discoveries of modern authors by robert morton\n",
      "\n",
      "1213701\n"
     ]
    }
   ],
   "source": [
    "# check for files that are bigger than 1000000 and if so, copy to another folder\n",
    "for file in os.listdir(eicFolder):\n",
    "    entries = pd.read_csv(eicFolder + \"\\\\\" + file)\n",
    "    num_rows = entries.index\n",
    "    num_texts = len(num_rows)\n",
    "    for i in range(num_texts):\n",
    "        data = entries[i:(i+1)]\n",
    "        name = tooBig + \"\\\\\" + file\n",
    "        text = data.iloc[0].text\n",
    "        title = data.iloc[0].title\n",
    "        if (len(text)>=1000000):\n",
    "            print(title+\"\\n\")\n",
    "            print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "18770509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#break the too big file into two smaller ones \n",
    "\n",
    "breakFile = r'C:\\Users\\amycw\\Desktop\\ecbc research\\eic_monopoly_eebo\\A5_P4_435.csv'\n",
    "entriesOg = pd.read_csv(breakFile)\n",
    "half = []\n",
    "otherHalf = []\n",
    "count = 0 \n",
    "string = \"\"\n",
    "other = \"\"\n",
    "\n",
    "num_texts = len(entriesOg.index)\n",
    "for i in range(num_texts):\n",
    "    d1 = entriesOg[i:(i+1)]\n",
    "    name = eicFolder + \"\\\\\" + file+\"_1\"\n",
    "    text = d1.iloc[0].text\n",
    "    words = text.split(\" \")\n",
    "    for w in words: \n",
    "        if (count <= (len(words)/2)):\n",
    "            half.append(w)\n",
    "            count+=1\n",
    "        else:\n",
    "            otherHalf.append(w)\n",
    "for w in half:\n",
    "    string = string + w + \" \"\n",
    "for w in otherHalf: \n",
    "    other = other + w + \" \"\n",
    "\n",
    "df1 = pd.DataFrame([(d1.iloc[0].index, d1.iloc[0].title,d1.iloc[0].author,d1.iloc[0].publisher,d1.iloc[0].date,string)],\n",
    "           columns=('index', 'title','author','publisher','date','text'))\n",
    "df2 = pd.DataFrame([(d1.iloc[0].index, d1.iloc[0].title,d1.iloc[0].author,d1.iloc[0].publisher,d1.iloc[0].date,other)],\n",
    "           columns=('index', 'title','author','publisher','date','text'))\n",
    "df1.to_csv(eicFolder + \"\\\\\" + 'A5_P4_435_1.csv', index=False)\n",
    "df2.to_csv(eicFolder + \"\\\\\" + 'A5_P4_435_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "507b3c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "nerFile = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\namedEntityRecognition.txt', \"a\")\n",
    "nerFile.write(\"format is (name type, frequency)\\n\")\n",
    "ner = spacy.load('en_core_web_sm')\n",
    "count=0\n",
    "for fileName in os.listdir(eicFolder):\n",
    "    readFile = pd.read_csv(eicFolder + \"\\\\\" + fileName)\n",
    "    num_texts = len(readFile.index)\n",
    "    for i in range(num_texts):\n",
    "        data = readFile[i:(i+1)]\n",
    "        text = data.iloc[0].text\n",
    "        entities = ner(text)\n",
    "        for ent in entities.ents:\n",
    "            nerFile.write(ent.text+\" \"+ent.label_+\"\\n\")\n",
    "        count+=1    \n",
    "        print(count)\n",
    "nerFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "526c4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabetize and sort entities by frequencies \n",
    "\n",
    "nerFile = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\namedEntityRecognition.txt', \"r\")\n",
    "SortedNER = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\sortedNamedEntityRecognition.txt',\"a\")\n",
    "SortedNER.write(\"format is (name type, frequency)\\n\")\n",
    "uniqueEnts = {} \n",
    "entities = []\n",
    "indices = []\n",
    "entities = nerFile.read().split(\"\\n\")\n",
    "for ent in entities:\n",
    "    if ent not in indices: \n",
    "        uniqueEnts[ent] = 1\n",
    "        indices.append(ent)\n",
    "    else: \n",
    "        uniqueEnts[ent] = uniqueEnts[ent] + 1 \n",
    "alphabetizedEnts = {}\n",
    "for ent in sorted(uniqueEnts): \n",
    "    alphabetizedEnts[ent]=uniqueEnts[ent] \n",
    "for pair in sorted (alphabetizedEnts.items(), key=lambda kv:(kv[1], kv[0]), reverse=True):\n",
    "    SortedNER.write(str(pair)+\"\\n\")\n",
    "SortedNER.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a57cc0fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1449451 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-19008bcfbc4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadFile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mentities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    999\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m         \"\"\"\n\u001b[1;32m-> 1001\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1002\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1089\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1092\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE866\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1079\u001b[0m         \"\"\"\n\u001b[0;32m   1080\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1082\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: [E088] Text of length 1449451 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "# failed attempt \n",
    "nerFile = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\monopoly_NER.txt', \"a\")\n",
    "nerFile.write(\"format is (name type, frequency)\\n\")\n",
    "\n",
    "ner = spacy.load('en_core_web_sm')\n",
    "uniqueEnts = {} \n",
    "for fileName in os.listdir(monopolyFolder):\n",
    "    readFile = pd.read_csv(monopolyFolder + \"\\\\\" + fileName)\n",
    "    num_rows = readFile.index\n",
    "    num_texts = len(num_rows)\n",
    "    for i in range(num_texts):\n",
    "        data = readFile[i:(i+1)]\n",
    "        text = data.iloc[0].text\n",
    "        entities = ner(text)\n",
    "        for ent in entities.ents:\n",
    "            idx = ent.text+\" \"+ent.label_\n",
    "            if ent not in uniqueEnts: \n",
    "                uniqueEnts[idx] = 1\n",
    "            else: \n",
    "                uniqueEnts[idx] += 1 \n",
    "alphabetizedEnts = {}\n",
    "for ent in sorted(uniqueEnts): \n",
    "    alphabetizedEnts[ent]=uniqueEnts[ent] \n",
    "for pair in sorted (alphabetizedEnts.items(), key=lambda kv:(kv[1], kv[0]), reverse=True):\n",
    "    nerFile.write(str(pair)+\"\\n\")\n",
    "nerFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
