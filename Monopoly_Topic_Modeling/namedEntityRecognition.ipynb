{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c71dbcb",
   "metadata": {},
   "source": [
    "This code is for running named entity recognition (NER) using spaCy on individual Restoration-era texts that mention monopoly, East India Company, and other terms we are interested in. \n",
    "* NER: https://www.geeksforgeeks.org/python-named-entity-recognition-ner-using-spacy/\n",
    "* Sorting by val: https://www.geeksforgeeks.org/python-sort-python-dictionaries-by-key-or-value/\n",
    "\n",
    "CHALLENGES: I tried to run the algorithm on the entire monopolyFolder, but NER w/ spaCy has a max character limit of 1 million. The code ran for nearly 20 minutes before failing. What I tried next was to filter through the texts further for mentions of the East India Company to narrow down the number of texts, which resulted in 24 texts. One of the texts had more than 1 million characters, so I split it in half into two. I then ran NER on the eicFolder. I sorted the NER output by frequency and in alphabetical order. \n",
    "\n",
    "other notes: \n",
    "* every single Restoration monopoly EIC file (24 in total; the medCount prints 25 because I divided one file into two) contains some mentions of words within my  medical lexicon \n",
    "\n",
    "TO FIX: \n",
    "* THERE ARE QUITE A FEW REDUNDANT CSV FILES IN ALL THE FOLDERS (SAME TEXTS), which means that there is double counting in both the NER and the medical terms word cloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9417b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import sys\n",
    "# !{sys.executable} -m pip install spacy\n",
    "# !{sys.executable} -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "directory = r'C:\\Users\\amycw\\Desktop\\ecbc research'\n",
    "monopolyFolder = directory + \"\\\\\" + 'monopoly_eebo'\n",
    "tooBig = directory + \"\\\\\" + 'tooBig'\n",
    "eicFolder = directory + \"\\\\\"+'eic_monopoly_eebo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c82632b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eicCount = 0;\n",
    "\n",
    "for name in os.listdir(monopolyFolder):\n",
    "    readFile = pd.read_csv(monopolyFolder + \"\\\\\" + name)\n",
    "    num_rows = readFile.index\n",
    "    num_texts = len(num_rows)\n",
    "    for i in range(num_texts):\n",
    "        data = readFile[i:(i+1)]\n",
    "        text = data.iloc[0].text\n",
    "        eic = re.compile(\"East India Company|east india company|east indies\")\n",
    "        if (re.search(eic, text) != None):\n",
    "#             moveFile = eicFolder + \"\\\\\" + name\n",
    "#             data.to_csv(moveFile, index=False)\n",
    "            eicCount +=1\n",
    "print(str(eicCount)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96bceee5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tan answer of the company of royal adventurers of england trading into africa to the petition and paper of certain heads and particulars thereunto relating and annexed exhibited to the honourable house of commons by sir paul painter ferdinand gorges henry batson benjamin skutt and thomas knights on the behalf of themselves and others concerned in his majesties plantations in america\n",
      "\n",
      "A25_P5_112.csv\n",
      "\n",
      "\tthe argument of a learned counsel upon an action of the case brought by the east-india-company against mr thomas sands an interloper\n",
      "\n",
      "A25_P5_231.csv\n",
      "\n",
      "\tan answer of the company of royal adventurers of england trading into africa to the petition and paper of certain heads and particulars thereunto relating and annexed exhibited to the honourable house of commons by sir paul painter ferdinando gorges henry batson benjamin skutt and thomas knights on the behalf of themselves and others concerned in his majesties plantations in america\n",
      "\n",
      "A2_P4_1058.csv\n",
      "\n",
      "\tthe character of queen elizabeth or a full and clear account of her policies and the methods of her government both in church and state her virtue and defects together with the characters of her principal ministers of state and the greatest part of the affairs and events that happened in her times collected and faithfully represented by edmund bohun esquire\n",
      "\n",
      "A2_P5_phase1_1909.csv\n",
      "\n",
      "\tan essay towards a scheme or model for erecting a national east-india joint-stock or company more generally diffused and enlarged for the restoring establishing and better carrying on that most important trade fully discoursed in a letter to a person of quality\n",
      "\n",
      "A38_P5_183.csv\n",
      "\n",
      "\taeternalia or a treatise wherein by way of explication demonstration confirmation and application is showed that the great labour and pains of every christian ought chiefly to be employed not about perishing but eternal good things from john by francis craven\n",
      "\n",
      "A3_P4_1151.csv\n",
      "\n",
      "\tan essay towards a scheme or model for erecting a national east-india joynt-stock or company more generally diffused and enlarged for the restoring establishing and better carrying on that most important trade fully discoursed in a letter to a person of quality\n",
      "\n",
      "A3_P4_2202.csv\n",
      "\n",
      "\tfree regulated trade particularly to india the interest of england being the true natural means to promote the navigation and riches of this nation forts and castles in india notwithstanding all specious pretences are occasionally prov' to be of uncertain advantage but of certain inconvenience to us discours' in a letter to a friend\n",
      "\n",
      "A4_P4_155.csv\n",
      "\n",
      "\ta journal of several remarkable passages before the honourable house of commons and the right honourable the lords of their majesties most honourable privy council relating to the east-india trade\n",
      "\n",
      "A4_P4_1747.csv\n",
      "\n",
      "\tplain dealing in a dialogue between mr johnson and mr wary his friend a stock-jobber and a petitoner against the e-- i-- company about stock-jobbing and the said company\n",
      "\n",
      "A4_P4_1972.csv\n",
      "\n",
      "\tthe interest of ireland in its trade and wealth stated in two parts first part observes and discovers the causes of irelands not more increasing in trade and wealth from the first conquest till now second part proposes expedients to remedy all its mercanture maladies and other wealth-wasting enormities by which it is kept poor and low both ' with some observations on the politics of government relating to the encouragement of trade and increase of wealth with some reflections on principles of religion as it relates to the premises by richard lawrence \n",
      "\n",
      "A4_P4_2918.csv\n",
      "\n",
      "\tdiscourse of trade coin and paper credit and of ways and means to gain and retain riches to which is added the argument of a learned counsel upon an action of a case brought by the east-india-company against mr sands the interloper\n",
      "\n",
      "A55_P5_51.csv\n",
      "\n",
      "\tof trade in general in particular domestic foreign the east-india the african the turkey the spanish the hamburgh the portugal the italian the dutch the russia the greenland the swedeland the denmark the irish the scotland the plantation the french also of coin bullion of improving our woollen manufacture to prevent exporting wool of ways and means to increase our riches by jp esq to which is annex' the argument of the late lord chief justice pollexphen upon an action of the case brought by the east-india company against mr sands an interloper\n",
      "\n",
      "A55_P5_53.csv\n",
      "\n",
      "\ta voyage to suratt in the year giving a large account of that city and its inhabitants and of the english factory there likewise a description of madiera st jago annobon cabenda and malemba upon the coast of by j ovington\n",
      "\n",
      "A5_P4_1219.csv\n",
      "\n",
      "\tsamaritanism reviv' a sermon preached at the parish church of great yarmouth upon the ninth of september being the day appointed for a solemn thanksgiving for the discovery of the late horrid plot against his majesty's person and government by luke melbourne\n",
      "\n",
      "A5_P4_303.csv\n",
      "\n",
      "\tgeography rectified or a description of the world in all its kingdoms provinces countries islands cities towns seas rivers baize capes ports their ancient and present names inhabitants situations histories customs governments as also their commodities coins weights and measures compared with those at london illustrated with seventy six maps the whole work performed according to the more accurate observations and discoveries of modern authors by robert morton\n",
      "\n",
      "A5_P4_435_1.csv\n",
      "\n",
      "\tgeography rectified or a description of the world in all its kingdoms provinces countries islands cities towns seas rivers baize capes ports their ancient and present names inhabitants situations histories customs governments as also their commodities coins weights and measures compared with those at london illustrated with seventy six maps the whole work performed according to the more accurate observations and discoveries of modern authors by robert morton\n",
      "\n",
      "A5_P4_435_2.csv\n",
      "\n",
      "\ta letter to a lord concerning a bill to incorporate the old east-india company\n",
      "\n",
      "A5_P4_939.csv\n",
      "\n",
      "\tthe happy union of england and holland or the advantageous consequences of the alliance of the crown of great britain with the states general of the united provinces\n",
      "\n",
      "A6_P4_2298.csv\n",
      "\n",
      "\tthe reading upon the statute of the thirteenth of elizabeth chapter touching bankrupts learnedly and amply expained by john stone of gray's inn esquire\n",
      "\n",
      "A9_P4_1941.csv\n",
      "\n",
      "\tthe irregular and disorderly state of the plantation-trade discuss' and humbly offered to the consideration of the right honourable the lords and commons in parliament assembled\n",
      "\n",
      "B0_P4_1030.csv\n",
      "\n",
      "\trelazione della corte roman fatta 'anno english\n",
      "\n",
      "B2_P4_113.csv\n",
      "\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# medCount = 0; \n",
    "allMedTerms = []\n",
    "titles = []\n",
    "num = 0\n",
    "ignore = \"geography rectified or a description of the world in all its kingdoms provinces countries islands cities towns seas rivers baize capes ports their ancient and present names inhabitants situations histories customs governments as also their commodities coins weights and measures compared with those at london illustrated with seventy six maps the whole work performed according to the more accurate observations and discoveries of modern authors by robert morton\"\n",
    "\n",
    "for name in os.listdir(eicFolder):\n",
    "    readFile = pd.read_csv(eicFolder + \"\\\\\" + name)\n",
    "    num_rows = readFile.index\n",
    "    num_texts = len(num_rows)\n",
    "    for i in range(num_texts):\n",
    "        data = readFile[i:(i+1)]\n",
    "        text = data.iloc[0].text\n",
    "        title = data.iloc[0].title\n",
    "        if title in titles: \n",
    "            continue\n",
    "        else: \n",
    "            print(\"\\t\" + title + \"\\n\")\n",
    "            print(name + \"\\n\")\n",
    "            lexicon = re.compile(\"corruption|consumption|body politick|corrupt|consume|wasting|waste|blood|physician|decay|body|canker|disease|illness|remedy|cure|sickness|hepatitis|fever|air|life|death|spirit|brain|mind|vital|humor|longevity|therapeutic|nature|degeneration|degenerate|plague|smells|putrid|bad|tetrid|breathing|wholesome|healthy|unhealthy|sane|insane|nervous|languish|faculties|enfeeblement|drinks|tuberculosis|constitution|bile|black bile|yellow bile|phlegm|lung|sores|fog|smoke|diagnosis|prognosis|fiber|atrophy|morbid|mortal|mortality|nerves|inanition|defect|distemper|swelling|upset|stomach|cough|exercise|unwholesome|diet|evacuation|fatal|fatality|vessels|hemmorrhage|bleeding|bleed|melancholy|diabetes|asthma|sweat|serum|water|fire|vomit|opiate|opium|ulcer|envy|jealousy|spoil|liver|vein|wear|joint|supple|heart|mouth|shaking|urine|cured|pox\")\n",
    "            if (re.search(lexicon, text) != None):\n",
    "                for w in (re.findall(lexicon, text)):\n",
    "                    allMedTerms.append(w)\n",
    "            if title != ignore:\n",
    "                titles.append(title)\n",
    "            num+=1\n",
    "        \n",
    "#             print(\"to delete:\" + name + \"\\n\" + title + \"\\n\")\n",
    "print(num)\n",
    "#             medCount += 1\n",
    "# print(str(medCount)+\"\\n\")\n",
    "# print(allMedTerms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79fe4bdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('air', 1156)\n",
      "\n",
      "('life', 786)\n",
      "\n",
      "('water', 585)\n",
      "\n",
      "('death', 511)\n",
      "\n",
      "('body', 473)\n",
      "\n",
      "('cure', 423)\n",
      "\n",
      "('spirit', 411)\n",
      "\n",
      "('heart', 406)\n",
      "\n",
      "('nature', 374)\n",
      "\n",
      "('mind', 306)\n",
      "\n",
      "('blood', 296)\n",
      "\n",
      "('liver', 288)\n",
      "\n",
      "('fire', 242)\n",
      "\n",
      "('mouth', 215)\n",
      "\n",
      "('bad', 211)\n",
      "\n",
      "('wear', 205)\n",
      "\n",
      "('joint', 195)\n",
      "\n",
      "('constitution', 143)\n",
      "\n",
      "('mortal', 131)\n",
      "\n",
      "('exercise', 100)\n",
      "\n",
      "('remedy', 91)\n",
      "\n",
      "('corrupt', 82)\n",
      "\n",
      "('decay', 81)\n",
      "\n",
      "('spoil', 78)\n",
      "\n",
      "('waste', 71)\n",
      "\n",
      "('diet', 69)\n",
      "\n",
      "('vessels', 58)\n",
      "\n",
      "('defect', 58)\n",
      "\n",
      "('disease', 56)\n",
      "\n",
      "('physician', 48)\n",
      "\n",
      "('consume', 47)\n",
      "\n",
      "('fatal', 44)\n",
      "\n",
      "('plague', 40)\n",
      "\n",
      "('sickness', 31)\n",
      "\n",
      "('corruption', 30)\n",
      "\n",
      "('distemper', 29)\n",
      "\n",
      "('consumption', 29)\n",
      "\n",
      "('sweat', 28)\n",
      "\n",
      "('jealousy', 27)\n",
      "\n",
      "('envy', 26)\n",
      "\n",
      "('stomach', 25)\n",
      "\n",
      "('degenerate', 25)\n",
      "\n",
      "('brain', 25)\n",
      "\n",
      "('bile', 25)\n",
      "\n",
      "('wholesome', 24)\n",
      "\n",
      "('melancholy', 24)\n",
      "\n",
      "('languish', 20)\n",
      "\n",
      "('fever', 20)\n",
      "\n",
      "('vein', 17)\n",
      "\n",
      "('lung', 17)\n",
      "\n",
      "('smoke', 15)\n",
      "\n",
      "('healthy', 15)\n",
      "\n",
      "('swelling', 14)\n",
      "\n",
      "('supple', 13)\n",
      "\n",
      "('fog', 12)\n",
      "\n",
      "('wasting', 11)\n",
      "\n",
      "('unwholesome', 11)\n",
      "\n",
      "('smells', 10)\n",
      "\n",
      "('vomit', 9)\n",
      "\n",
      "('faculties', 7)\n",
      "\n",
      "('drinks', 7)\n",
      "\n",
      "('breathing', 7)\n",
      "\n",
      "('sores', 6)\n",
      "\n",
      "('opium', 6)\n",
      "\n",
      "('urine', 5)\n",
      "\n",
      "('vital', 4)\n",
      "\n",
      "('shaking', 4)\n",
      "\n",
      "('sane', 4)\n",
      "\n",
      "('pox', 4)\n",
      "\n",
      "('nerves', 4)\n",
      "\n",
      "('cough', 4)\n",
      "\n",
      "('bleeding', 4)\n",
      "\n",
      "('unhealthy', 2)\n",
      "\n",
      "('ulcer', 2)\n",
      "\n",
      "('putrid', 2)\n",
      "\n",
      "('phlegm', 2)\n",
      "\n",
      "('canker', 2)\n",
      "\n",
      "('serum', 1)\n",
      "\n",
      "('humor', 1)\n",
      "\n",
      "('bleed', 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uniqueMed = {} \n",
    "terms = []\n",
    "for w in allMedTerms:\n",
    "    if w not in terms: \n",
    "        uniqueMed[w] = 1\n",
    "        terms.append(w)\n",
    "    else: \n",
    "        uniqueMed[w] = uniqueMed[w] + 1 \n",
    "\n",
    "for pair in sorted (uniqueMed.items(), key=lambda kv:(kv[1], kv[0]), reverse=True):\n",
    "    print(str(pair)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b6c6d6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geography rectified or a description of the world in all its kingdoms provinces countries islands cities towns seas rivers baize capes ports their ancient and present names inhabitants situations histories customs governments as also their commodities coins weights and measures compared with those at london illustrated with seventy six maps the whole work performed according to the more accurate observations and discoveries of modern authors by robert morton\n",
      "\n",
      "1213701\n"
     ]
    }
   ],
   "source": [
    "# check for files that are bigger than 1000000 and if so, copy to another folder\n",
    "for file in os.listdir(eicFolder):\n",
    "    entries = pd.read_csv(eicFolder + \"\\\\\" + file)\n",
    "    num_rows = entries.index\n",
    "    num_texts = len(num_rows)\n",
    "    for i in range(num_texts):\n",
    "        data = entries[i:(i+1)]\n",
    "        name = tooBig + \"\\\\\" + file\n",
    "        text = data.iloc[0].text\n",
    "        title = data.iloc[0].title\n",
    "        if (len(text)>=1000000):\n",
    "            print(title+\"\\n\")\n",
    "            print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "18770509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#break the too big file into two smaller ones \n",
    "\n",
    "breakFile = r'C:\\Users\\amycw\\Desktop\\ecbc research\\eic_monopoly_eebo\\A5_P4_435.csv'\n",
    "entriesOg = pd.read_csv(breakFile)\n",
    "half = []\n",
    "otherHalf = []\n",
    "count = 0 \n",
    "string = \"\"\n",
    "other = \"\"\n",
    "\n",
    "num_texts = len(entriesOg.index)\n",
    "for i in range(num_texts):\n",
    "    d1 = entriesOg[i:(i+1)]\n",
    "    name = eicFolder + \"\\\\\" + file+\"_1\"\n",
    "    text = d1.iloc[0].text\n",
    "    words = text.split(\" \")\n",
    "    for w in words: \n",
    "        if (count <= (len(words)/2)):\n",
    "            half.append(w)\n",
    "            count+=1\n",
    "        else:\n",
    "            otherHalf.append(w)\n",
    "for w in half:\n",
    "    string = string + w + \" \"\n",
    "for w in otherHalf: \n",
    "    other = other + w + \" \"\n",
    "\n",
    "df1 = pd.DataFrame([(d1.iloc[0].index, d1.iloc[0].title,d1.iloc[0].author,d1.iloc[0].publisher,d1.iloc[0].date,string)],\n",
    "           columns=('index', 'title','author','publisher','date','text'))\n",
    "df2 = pd.DataFrame([(d1.iloc[0].index, d1.iloc[0].title,d1.iloc[0].author,d1.iloc[0].publisher,d1.iloc[0].date,other)],\n",
    "           columns=('index', 'title','author','publisher','date','text'))\n",
    "df1.to_csv(eicFolder + \"\\\\\" + 'A5_P4_435_1.csv', index=False)\n",
    "df2.to_csv(eicFolder + \"\\\\\" + 'A5_P4_435_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "507b3c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "nerFile = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\namedEntityRecognition.txt', \"a\")\n",
    "nerFile.write(\"format is (name type, frequency)\\n\")\n",
    "ner = spacy.load('en_core_web_sm')\n",
    "count=0\n",
    "titles = []\n",
    "\n",
    "for fileName in os.listdir(eicFolder):\n",
    "    readFile = pd.read_csv(eicFolder + \"\\\\\" + fileName)\n",
    "    num_texts = len(readFile.index)\n",
    "    for i in range(num_texts):\n",
    "        data = readFile[i:(i+1)]\n",
    "        text = data.iloc[0].text\n",
    "        title = data.iloc[0].title\n",
    "        if title not in titles:\n",
    "            entities = ner(text)\n",
    "            titles.append(title)\n",
    "            for ent in entities.ents:\n",
    "                nerFile.write(ent.text+\" \"+ent.label_+\"\\n\")\n",
    "        count+=1    \n",
    "        print(count)\n",
    "nerFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "526c4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabetize and sort entities by frequencies \n",
    "\n",
    "nerFile = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\namedEntityRecognition.txt', \"r\")\n",
    "SortedNER = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\sortedNamedEntityRecognition.txt',\"a\")\n",
    "SortedNER.write(\"format is (name type, frequency)\\n\")\n",
    "uniqueEnts = {} \n",
    "entities = []\n",
    "indices = []\n",
    "entities = nerFile.read().split(\"\\n\")\n",
    "for ent in entities:\n",
    "    if ent not in indices: \n",
    "        uniqueEnts[ent] = 1\n",
    "        indices.append(ent)\n",
    "    else: \n",
    "        uniqueEnts[ent] = uniqueEnts[ent] + 1 \n",
    "alphabetizedEnts = {}\n",
    "for ent in sorted(uniqueEnts): \n",
    "    alphabetizedEnts[ent]=uniqueEnts[ent] \n",
    "for pair in sorted (alphabetizedEnts.items(), key=lambda kv:(kv[1], kv[0]), reverse=True):\n",
    "    SortedNER.write(str(pair)+\"\\n\")\n",
    "SortedNER.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a57cc0fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1449451 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-19008bcfbc4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadFile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mentities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    999\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m         \"\"\"\n\u001b[1;32m-> 1001\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1002\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1089\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1092\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE866\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1079\u001b[0m         \"\"\"\n\u001b[0;32m   1080\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1082\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: [E088] Text of length 1449451 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "# failed attempt \n",
    "nerFile = open(r'C:\\Users\\amycw\\Desktop\\ecbc research\\monopoly_NER.txt', \"a\")\n",
    "nerFile.write(\"format is (name type, frequency)\\n\")\n",
    "\n",
    "ner = spacy.load('en_core_web_sm')\n",
    "uniqueEnts = {} \n",
    "for fileName in os.listdir(monopolyFolder):\n",
    "    readFile = pd.read_csv(monopolyFolder + \"\\\\\" + fileName)\n",
    "    num_rows = readFile.index\n",
    "    num_texts = len(num_rows)\n",
    "    for i in range(num_texts):\n",
    "        data = readFile[i:(i+1)]\n",
    "        text = data.iloc[0].text\n",
    "        entities = ner(text)\n",
    "        for ent in entities.ents:\n",
    "            idx = ent.text+\" \"+ent.label_\n",
    "            if ent not in uniqueEnts: \n",
    "                uniqueEnts[idx] = 1\n",
    "            else: \n",
    "                uniqueEnts[idx] += 1 \n",
    "alphabetizedEnts = {}\n",
    "for ent in sorted(uniqueEnts): \n",
    "    alphabetizedEnts[ent]=uniqueEnts[ent] \n",
    "for pair in sorted (alphabetizedEnts.items(), key=lambda kv:(kv[1], kv[0]), reverse=True):\n",
    "    nerFile.write(str(pair)+\"\\n\")\n",
    "nerFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
